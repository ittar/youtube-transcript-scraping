{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from string import punctuation\n",
    "import glob\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_sentence(text: str):\n",
    "    sentence_pattern = r'.*[.!?]$'\n",
    "    return bool(re.search(sentence_pattern, text.strip()))\n",
    "\n",
    "def clean(text: str):\n",
    "    punctuation_pattern = re.compile(f\"[{punctuation}]\")\n",
    "    spaces_pattern = re.compile(r\"\\s+\")\n",
    "    non_ascii_pattern = re.compile(r\"[^\\x00-\\x7F\\u0E00-\\u0E7F]\")\n",
    "    text = (\n",
    "        text.replace(\"'t\", \" not \")\n",
    "        .replace(\"’t\", \" not \")\n",
    "        .replace(\"'s\", \" is \")\n",
    "        .replace(\"’s\", \" is \")\n",
    "        .replace(\"'re\", \" are \")\n",
    "        .replace(\"’re\", \" are \")\n",
    "        .replace(\"'m\", \" am \")\n",
    "        .replace(\"’m\", \" am \")\n",
    "        .replace(\"'ll\", \" will \")\n",
    "        .replace(\"’ll\", \" will \")\n",
    "        .replace(\"\\n\", \" \")\n",
    "    )\n",
    "    text = re.sub(punctuation_pattern, \" \", text)\n",
    "    text = re.sub(non_ascii_pattern, \" \", text)\n",
    "    return re.sub(spaces_pattern, \" \", text).strip()\n",
    "\n",
    "def format(data):\n",
    "    thai_sentences = list()\n",
    "    eng_sentences = list()\n",
    "    thai_sentence = ''\n",
    "    eng_sentence = ''\n",
    "    j = 0\n",
    "    if ('th' in data['transcript'] and 'en' in data['transcript']):\n",
    "        for i, th in enumerate(data['transcript']['th']):\n",
    "            if (j >= len(data['transcript']['en'])): break\n",
    "            en = data['transcript']['en'][j]\n",
    "            en_text = en['text']\n",
    "            en_start = en['start']  \n",
    "            en_duration = en['duration']\n",
    "            th_text = th['text']\n",
    "            th_start = th['start']\n",
    "            th_duration = th['duration']\n",
    "            if (abs(en_start - th_start) < 0.5):\n",
    "                thai_sentence += ' ' + th_text\n",
    "                eng_sentence += ' ' + en_text\n",
    "                if (is_sentence(eng_sentence)):\n",
    "                    eng_sentences.append(clean(eng_sentence))\n",
    "                    thai_sentences.append(clean(thai_sentence))\n",
    "                    thai_sentence = ''\n",
    "                    eng_sentence = ''\n",
    "                j+=1\n",
    "    elif ('en' in data['transcript']):\n",
    "        for i, en in enumerate(data['transcript']['en']):\n",
    "            en_text = en['text']\n",
    "            en_start = en['start']  \n",
    "            en_duration = en['duration']\n",
    "            eng_sentence += ' ' + en_text\n",
    "            if (is_sentence(eng_sentence)):\n",
    "                eng_sentences.append(clean(eng_sentence))\n",
    "                eng_sentence = ''\n",
    "    return thai_sentences, eng_sentences\n",
    "\n",
    "def _compare_print(ths, ens):\n",
    "    for th,en in zip(ths,ens):\n",
    "        print(en)\n",
    "        print(th)\n",
    "        print('-'*10)\n",
    "\n",
    "def read_transcripts(channel):\n",
    "    os.makedirs(f'all_transcript/{channel}/format', exist_ok=True)\n",
    "    for file_path in glob.glob(f'all_transcript/{channel}/transcripts/*.json'):\n",
    "        with open(file_path, 'r') as file:\n",
    "            json_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "            data = json.load(file)\n",
    "            ths, ens = format(data)\n",
    "            if len(ths) != len(ens):\n",
    "                max_len = max(len(ths), len(ens))\n",
    "                ths += [''] * (max_len - len(ths)) \n",
    "                ens += [''] * (max_len - len(ens)) \n",
    "            df = pd.DataFrame({'en': ens, 'th': ths})\n",
    "            df.to_csv(f'all_transcript/{channel}/format/{json_name}.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_transcripts('khan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# with open('all_transcript/ted-ed/transcripts/-EG6rqA2vvA.json', 'r') as file:\n",
    "#             data = json.load(file)\n",
    "#             ths, ens = format(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "youtube",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
